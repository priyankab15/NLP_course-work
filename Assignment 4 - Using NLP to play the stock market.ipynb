{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long</th>\n",
       "      <th>mid</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>{'20040106': -0.0023, '20040107': -0.0016, '20...</td>\n",
       "      <td>{'20040106': 0.06760000000000001, '20040107': ...</td>\n",
       "      <td>{'20040106': -0.0013000000000000002, '20040107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABB</th>\n",
       "      <td>{'20040106': 0.09630000000000001, '20040107': ...</td>\n",
       "      <td>{'20040106': 0.09340000000000001, '20040107': ...</td>\n",
       "      <td>{'20040106': 0.0015, '20040107': -0.0107000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABMD</th>\n",
       "      <td>{'20040106': 0.08360000000000001, '20040107': ...</td>\n",
       "      <td>{'20040106': 0.039400000000000004, '20040107':...</td>\n",
       "      <td>{'20040106': 0.0102, '20040107': 0.0217, '2004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABR</th>\n",
       "      <td>{'20040413': 0.0367, '20040414': 0.0053, '2004...</td>\n",
       "      <td>{'20040413': 0.0082, '20040414': 0.01970000000...</td>\n",
       "      <td>{'20040413': 0.013900000000000001, '20040414':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACAD</th>\n",
       "      <td>{'20040602': -0.049300000000000004, '20040603'...</td>\n",
       "      <td>{'20040602': -0.0821, '20040603': -0.0611, '20...</td>\n",
       "      <td>{'20040602': -0.0346, '20040603': -0.0005, '20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   long  \\\n",
       "AAPL  {'20040106': -0.0023, '20040107': -0.0016, '20...   \n",
       "ABB   {'20040106': 0.09630000000000001, '20040107': ...   \n",
       "ABMD  {'20040106': 0.08360000000000001, '20040107': ...   \n",
       "ABR   {'20040413': 0.0367, '20040414': 0.0053, '2004...   \n",
       "ACAD  {'20040602': -0.049300000000000004, '20040603'...   \n",
       "\n",
       "                                                    mid  \\\n",
       "AAPL  {'20040106': 0.06760000000000001, '20040107': ...   \n",
       "ABB   {'20040106': 0.09340000000000001, '20040107': ...   \n",
       "ABMD  {'20040106': 0.039400000000000004, '20040107':...   \n",
       "ABR   {'20040413': 0.0082, '20040414': 0.01970000000...   \n",
       "ACAD  {'20040602': -0.0821, '20040603': -0.0611, '20...   \n",
       "\n",
       "                                                  short  \n",
       "AAPL  {'20040106': -0.0013000000000000002, '20040107...  \n",
       "ABB   {'20040106': 0.0015, '20040107': -0.0107000000...  \n",
       "ABMD  {'20040106': 0.0102, '20040107': 0.0217, '2004...  \n",
       "ABR   {'20040413': 0.013900000000000001, '20040414':...  \n",
       "ACAD  {'20040602': -0.0346, '20040603': -0.0005, '20...  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read reuters data\n",
    "news= pd.read_csv(\"news_reuters.csv\", header=0)\n",
    "news.columns=[\"ticker\", \"name_of_company\", \"date\", \"headline\", \"first_sentence\", \"news_category\"]\n",
    "stock_info= pd.read_json('stockReturns.json')\n",
    "stock_info.head()\n",
    "#stock_info_long= stock_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>name_of_company</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>first_sentence</th>\n",
       "      <th>news_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation</td>\n",
       "      <td>20110708</td>\n",
       "      <td>Global markets weekahead: Lacking conviction</td>\n",
       "      <td>LONDON Investors are unlikely to gain strong c...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation</td>\n",
       "      <td>20110708</td>\n",
       "      <td>Jobs halt Wall Street rally  investors eye ear...</td>\n",
       "      <td>NEW YORK Stocks fell on Friday as a weak jobs ...</td>\n",
       "      <td>topStory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation</td>\n",
       "      <td>20110708</td>\n",
       "      <td>REFILE-TABLE-Australia's top carbon polluters</td>\n",
       "      <td>CANBERRA  July 8 Following is a list of Austr...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation</td>\n",
       "      <td>20110708</td>\n",
       "      <td>US STOCKS-Jobs data hits stocks  but earnings ...</td>\n",
       "      <td>* Google slumps on downgrade  one of Nasdaq's...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation</td>\n",
       "      <td>20110708</td>\n",
       "      <td>US STOCKS-Jobs halt Wall St rally  investors e...</td>\n",
       "      <td>* Dow off 0.5 pct  S&amp;P down 0.7 pct  Nasdaq o...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker    name_of_company      date  \\\n",
       "0     AA  Alcoa Corporation  20110708   \n",
       "1     AA  Alcoa Corporation  20110708   \n",
       "2     AA  Alcoa Corporation  20110708   \n",
       "3     AA  Alcoa Corporation  20110708   \n",
       "4     AA  Alcoa Corporation  20110708   \n",
       "\n",
       "                                            headline  \\\n",
       "0      Global markets weekahead: Lacking conviction    \n",
       "1  Jobs halt Wall Street rally  investors eye ear...   \n",
       "2     REFILE-TABLE-Australia's top carbon polluters    \n",
       "3  US STOCKS-Jobs data hits stocks  but earnings ...   \n",
       "4  US STOCKS-Jobs halt Wall St rally  investors e...   \n",
       "\n",
       "                                      first_sentence news_category  \n",
       "0  LONDON Investors are unlikely to gain strong c...        normal  \n",
       "1  NEW YORK Stocks fell on Friday as a weak jobs ...      topStory  \n",
       "2   CANBERRA  July 8 Following is a list of Austr...        normal  \n",
       "3   * Google slumps on downgrade  one of Nasdaq's...        normal  \n",
       "4   * Dow off 0.5 pct  S&P down 0.7 pct  Nasdaq o...        normal  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>{'20040106': -0.0023, '20040107': -0.0016, '20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABB</td>\n",
       "      <td>{'20040106': 0.09630000000000001, '20040107': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>{'20040106': 0.08360000000000001, '20040107': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>{'20040413': 0.0367, '20040414': 0.0053, '2004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACAD</td>\n",
       "      <td>{'20040602': -0.049300000000000004, '20040603'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                               long\n",
       "0  AAPL  {'20040106': -0.0023, '20040107': -0.0016, '20...\n",
       "1   ABB  {'20040106': 0.09630000000000001, '20040107': ...\n",
       "2  ABMD  {'20040106': 0.08360000000000001, '20040107': ...\n",
       "3   ABR  {'20040413': 0.0367, '20040414': 0.0053, '2004...\n",
       "4  ACAD  {'20040602': -0.049300000000000004, '20040603'..."
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = stock_info['long']\n",
    "y = pd.DataFrame.from_dict(y)\n",
    "y.reset_index(inplace = True)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20040106</th>\n",
       "      <th>20040107</th>\n",
       "      <th>20040108</th>\n",
       "      <th>20040109</th>\n",
       "      <th>20040113</th>\n",
       "      <th>20040114</th>\n",
       "      <th>20040115</th>\n",
       "      <th>20040116</th>\n",
       "      <th>20040121</th>\n",
       "      <th>20040122</th>\n",
       "      <th>...</th>\n",
       "      <th>20180308</th>\n",
       "      <th>20180309</th>\n",
       "      <th>20180313</th>\n",
       "      <th>20180314</th>\n",
       "      <th>20180315</th>\n",
       "      <th>20180316</th>\n",
       "      <th>20180320</th>\n",
       "      <th>20180321</th>\n",
       "      <th>20180322</th>\n",
       "      <th>20180323</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.0023</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0376</td>\n",
       "      <td>-0.0423</td>\n",
       "      <td>-0.0556</td>\n",
       "      <td>-0.0741</td>\n",
       "      <td>-0.0411</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.0046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>-0.0069</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>-0.0378</td>\n",
       "      <td>-0.0684</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0155</td>\n",
       "      <td>-0.0069</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0836</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>-0.0199</td>\n",
       "      <td>-0.0829</td>\n",
       "      <td>-0.0392</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>-0.0115</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.0825</td>\n",
       "      <td>-0.1271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>-0.0239</td>\n",
       "      <td>-0.0415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0978</td>\n",
       "      <td>-0.1037</td>\n",
       "      <td>-0.3875</td>\n",
       "      <td>-0.3022</td>\n",
       "      <td>-0.2314</td>\n",
       "      <td>-0.2368</td>\n",
       "      <td>-0.2455</td>\n",
       "      <td>-0.2055</td>\n",
       "      <td>-0.2092</td>\n",
       "      <td>-0.2117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2718 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   20040106  20040107  20040108  20040109  20040113  20040114  20040115  \\\n",
       "0   -0.0023   -0.0016   -0.0376   -0.0423   -0.0556   -0.0741   -0.0411   \n",
       "1    0.0963    0.0916    0.1032   -0.0069    0.0404    0.0003   -0.0378   \n",
       "2    0.0836    0.0283   -0.0199   -0.0829   -0.0392    0.0117   -0.0115   \n",
       "3       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "4       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "   20040116  20040121  20040122    ...     20180308  20180309  20180313  \\\n",
       "0    0.0269    0.0021    0.0235    ...       0.0108    0.0045   -0.0034   \n",
       "1   -0.0684   -0.0017   -0.0395    ...      -0.0155   -0.0069    0.0214   \n",
       "2    0.0258   -0.0825   -0.1271    ...       0.0416    0.0403    0.0403   \n",
       "3       NaN       NaN       NaN    ...       0.0641    0.0431    0.0493   \n",
       "4       NaN       NaN       NaN    ...      -0.0978   -0.1037   -0.3875   \n",
       "\n",
       "   20180314  20180315  20180316  20180320  20180321  20180322  20180323  \n",
       "0    0.0019    0.0056    0.0052    0.0160    0.0209    0.0389    0.0046  \n",
       "1    0.0227    0.0138    0.0122    0.0087    0.0134    0.0206    0.0639  \n",
       "2    0.0640    0.0417    0.0388    0.0416    0.0387    0.0523    0.0492  \n",
       "3    0.0320    0.0204    0.0171   -0.0076   -0.0121   -0.0239   -0.0415  \n",
       "4   -0.3022   -0.2314   -0.2368   -0.2455   -0.2055   -0.2092   -0.2117  \n",
       "\n",
       "[5 rows x 2718 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = y['long'].apply(pd.Series)\n",
    "yy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>price value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040106</td>\n",
       "      <td>-0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040107</td>\n",
       "      <td>-0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040108</td>\n",
       "      <td>-0.0376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040109</td>\n",
       "      <td>-0.0423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040113</td>\n",
       "      <td>-0.0556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker      date  price value\n",
       "0   AAPL  20040106      -0.0023\n",
       "1   AAPL  20040107      -0.0016\n",
       "2   AAPL  20040108      -0.0376\n",
       "3   AAPL  20040109      -0.0423\n",
       "4   AAPL  20040113      -0.0556"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy['ticker'] = y['index']\n",
    "yy.set_index('ticker', inplace = True)\n",
    "yy = yy.stack()\n",
    "yy = yy.to_frame(name=None)\n",
    "yy.reset_index(inplace = True)\n",
    "yy.columns = ['ticker','date','price value']\n",
    "yy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>price value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker      date  price value\n",
       "0   AAPL  20040106            0\n",
       "1   AAPL  20040107            0\n",
       "2   AAPL  20040108            0\n",
       "3   AAPL  20040109            0\n",
       "4   AAPL  20040113            0"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy[\"price value\"]= (yy['price value'] > 0).astype(int)\n",
    "yy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439\n"
     ]
    }
   ],
   "source": [
    "print(len(set(yy[\"ticker\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224\n"
     ]
    }
   ],
   "source": [
    "print(len(set(news[\"ticker\"])))\n",
    "news_ticker= set(news[\"ticker\"])\n",
    "yy_ticker= set(yy[\"ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data= news.merge(yy, left_on='ticker', right_on='ticker', how='inner')\n",
    "df=merged_data.drop_duplicates(subset='first_sentence', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37040, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>name_of_company</th>\n",
       "      <th>date_x</th>\n",
       "      <th>headline</th>\n",
       "      <th>first_sentence</th>\n",
       "      <th>news_category</th>\n",
       "      <th>date_y</th>\n",
       "      <th>price value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK Apple Inc has made a \"promising start...</td>\n",
       "      <td>topStory</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>Apple antitrust compliance off to a promising ...</td>\n",
       "      <td>NEW YORK  April 14 Apple Inc has made a \"promi...</td>\n",
       "      <td>normal</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5436</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>COLUMN-How to avoid the trouble coming to the ...</td>\n",
       "      <td>(The opinions expressed here are those of the ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8154</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140414</td>\n",
       "      <td>How to avoid the trouble coming to the tech se...</td>\n",
       "      <td>CHICAGO A resounding shot across the bow has b...</td>\n",
       "      <td>normal</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>1-800 FLOWERSCOM Inc</td>\n",
       "      <td>20140415</td>\n",
       "      <td>Apple cannot escape U.S. states' e-book antitr...</td>\n",
       "      <td>NEW YORK Apple Inc on Tuesday lost an attempt ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker       name_of_company    date_x  \\\n",
       "0       AAPL  1-800 FLOWERSCOM Inc  20140414   \n",
       "2718    AAPL  1-800 FLOWERSCOM Inc  20140414   \n",
       "5436    AAPL  1-800 FLOWERSCOM Inc  20140414   \n",
       "8154    AAPL  1-800 FLOWERSCOM Inc  20140414   \n",
       "10872   AAPL  1-800 FLOWERSCOM Inc  20140415   \n",
       "\n",
       "                                                headline  \\\n",
       "0      Apple antitrust compliance off to a promising ...   \n",
       "2718   Apple antitrust compliance off to a promising ...   \n",
       "5436   COLUMN-How to avoid the trouble coming to the ...   \n",
       "8154   How to avoid the trouble coming to the tech se...   \n",
       "10872  Apple cannot escape U.S. states' e-book antitr...   \n",
       "\n",
       "                                          first_sentence news_category  \\\n",
       "0      NEW YORK Apple Inc has made a \"promising start...      topStory   \n",
       "2718   NEW YORK  April 14 Apple Inc has made a \"promi...        normal   \n",
       "5436   (The opinions expressed here are those of the ...        normal   \n",
       "8154   CHICAGO A resounding shot across the bow has b...        normal   \n",
       "10872  NEW YORK Apple Inc on Tuesday lost an attempt ...        normal   \n",
       "\n",
       "         date_y  price value  \n",
       "0      20040106            0  \n",
       "2718   20040106            0  \n",
       "5436   20040106            0  \n",
       "8154   20040106            0  \n",
       "10872  20040106            0  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df[\"first_sentence\"]\n",
    "Y= df[\"price value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37040/37040 [00:08<00:00, 4289.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apple', 'Inc', 'made', 'promising', 'start', 'enhancing', 'antitrust', 'compliance', 'program', 'found', 'liable', 'last', 'year', 'conspiring', 'raise', 'e', 'book', 'prices', 'work', 'required', 'court', 'appointed', 'monitor', 'said', 'Monday'], ['April', '14', 'Apple', 'Inc', 'made', 'promising', 'start', 'enhancing', 'antitrust', 'compliance', 'program', 'found', 'liable', 'last', 'year', 'conspiring', 'raise', 'e', 'book', 'prices', 'work', 'required', 'court', 'appointed', 'monitor', 'said', 'Monday'], ['The', 'opinions', 'expressed', 'author', 'columnist', 'Reuters'], ['A', 'resounding', 'shot', 'across', 'bow', 'fired', 'tech', 'sector', 'recent', 'weeks', 'The', 'tech', 'heavy', 'Nasdaq', 'Composite', 'Index', 'nearly', '5', 'percent', 'April', 'Friday', 'close', 'Nasdaq', 'Biotechnology', 'Index', '21', 'percent', 'record', 'closing', 'high', 'February', '25', 'Many', 'sector', 'flagships', 'newcomers', 'crosshairs'], ['Apple', 'Inc', 'Tuesday', 'lost', 'attempt', 'dismiss', 'lawsuits', 'state', 'attorneys', 'general', 'accusing', 'conspiring', 'five', 'major', 'publishers', 'fix', 'e', 'book', 'prices']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer,sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from string import punctuation, ascii_lowercase\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from geotext import GeoText\n",
    "stop_words = set(stopwords.words('english'))\n",
    "spec_char = ['~','!','@','#','$','%','^','&','*','(',')','_','+','|','}','{',\n",
    "                                               ':','\"',\"'\",'?','>','<','`','-','=',';','/','.',',','.)']\n",
    "loc= ['NEW','YORK','JERSEY']\n",
    "city=[]\n",
    "for sen in list_sentences:\n",
    "    places= GeoText(sen)\n",
    "    city.append(places.cities)\n",
    "city_flat_list = [item for sublist in city for item in sublist]\n",
    "city_list= list(set(city_flat_list))\n",
    "# setup tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def text_to_wordlist(text, lower=False):\n",
    "    \n",
    "    # Tokenize\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text= [t for t in text if t not in stop_words]\n",
    "    text= [c for c in text if c not in spec_char]\n",
    "    text= [l for l in text if l not in city_list]\n",
    "    text= [n for n in text if n not in loc]\n",
    "    \n",
    "    # Return a list of words\n",
    "    vocab.update(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_comments(list_sentences, lower=False):\n",
    "    comments = []\n",
    "    for text in tqdm(list_sentences):\n",
    "        txt = text_to_wordlist(text, lower=lower)\n",
    "        comments.append(txt)\n",
    "    return comments\n",
    "\n",
    "\n",
    "list_sentences = list(df[\"first_sentence\"].fillna(\"NAN_WORD\").values)\n",
    "comments = process_comments(list_sentences, lower=True)\n",
    "print(comments[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30046\n"
     ]
    }
   ],
   "source": [
    "print(len(set([item for sublist in comments for item in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      "LEXICON SAMPLE (30047 total items):\n",
      "{'Apple': 2, 'Inc': 3, 'made': 4, 'promising': 5, 'start': 6, 'enhancing': 7, 'antitrust': 8, 'compliance': 9, 'program': 10, 'found': 11, 'liable': 12, 'last': 13, 'year': 14, 'conspiring': 15, 'raise': 16, 'e': 17, 'book': 18, 'prices': 19, 'work': 20, 'required': 21}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def make_lexicon(token_seq, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seq:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "print(\"WORDS:\")\n",
    "words_lexicon = make_lexicon(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      " [[   0    0    0 ...   24   25   26]\n",
      " [   0    0    0 ...   24   25   26]\n",
      " [   0    0    0 ...   32   33   34]\n",
      " ...\n",
      " [   0    0    0 ...  227  228 3864]\n",
      " [   0    0    0 ...  227  228 3864]\n",
      " [   0    0    0 ... 3645  903  191]]\n",
      "SHAPE: (37040, 97) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "token_idx = tokens_to_idxs(comments, words_lexicon)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in token_idx]) # Get length of longest sequence\n",
    "train_padded_words = pad_idx_seqs(token_idx, \n",
    "                                  max_seq_len + 1)\n",
    "\n",
    "print(\"WORDS:\\n\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(train_padded_words,Y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25928 samples, validate on 11112 samples\n",
      "Epoch 1/100\n",
      "25928/25928 [==============================] - 112s 4ms/step - loss: 0.2728 - acc: 0.8864 - val_loss: 0.2070 - val_acc: 0.9195\n",
      "Epoch 2/100\n",
      "25928/25928 [==============================] - 109s 4ms/step - loss: 0.1237 - acc: 0.9518 - val_loss: 0.2056 - val_acc: 0.9268\n",
      "Epoch 3/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0773 - acc: 0.9711 - val_loss: 0.2338 - val_acc: 0.9240\n",
      "Epoch 4/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0539 - acc: 0.9803 - val_loss: 0.2563 - val_acc: 0.9179\n",
      "Epoch 5/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0442 - acc: 0.9826 - val_loss: 0.2623 - val_acc: 0.9244\n",
      "Epoch 6/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0375 - acc: 0.9859 - val_loss: 0.2881 - val_acc: 0.9232\n",
      "Epoch 7/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0334 - acc: 0.9871 - val_loss: 0.3240 - val_acc: 0.9212\n",
      "Epoch 8/100\n",
      "25928/25928 [==============================] - 106s 4ms/step - loss: 0.0268 - acc: 0.9892 - val_loss: 0.3388 - val_acc: 0.9213\n",
      "Epoch 9/100\n",
      "25928/25928 [==============================] - 106s 4ms/step - loss: 0.0255 - acc: 0.9893 - val_loss: 0.3833 - val_acc: 0.9175\n",
      "Epoch 10/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0249 - acc: 0.9902 - val_loss: 0.3625 - val_acc: 0.9263\n",
      "Epoch 11/100\n",
      "25928/25928 [==============================] - 109s 4ms/step - loss: 0.0219 - acc: 0.9910 - val_loss: 0.3916 - val_acc: 0.9217\n",
      "Epoch 12/100\n",
      "25928/25928 [==============================] - 120s 5ms/step - loss: 0.0199 - acc: 0.9914 - val_loss: 0.3958 - val_acc: 0.9212\n",
      "Epoch 13/100\n",
      "25928/25928 [==============================] - 113s 4ms/step - loss: 0.0199 - acc: 0.9914 - val_loss: 0.4133 - val_acc: 0.9220\n",
      "Epoch 14/100\n",
      "25928/25928 [==============================] - 114s 4ms/step - loss: 0.0192 - acc: 0.9918 - val_loss: 0.4153 - val_acc: 0.9218\n",
      "Epoch 15/100\n",
      "25928/25928 [==============================] - 114s 4ms/step - loss: 0.0170 - acc: 0.9924 - val_loss: 0.4326 - val_acc: 0.9227\n",
      "Epoch 16/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0164 - acc: 0.9925 - val_loss: 0.4105 - val_acc: 0.9244\n",
      "Epoch 17/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0163 - acc: 0.9927 - val_loss: 0.5060 - val_acc: 0.9233\n",
      "Epoch 18/100\n",
      "25928/25928 [==============================] - 112s 4ms/step - loss: 0.0163 - acc: 0.9929 - val_loss: 0.4793 - val_acc: 0.9177\n",
      "Epoch 19/100\n",
      "25928/25928 [==============================] - 112s 4ms/step - loss: 0.0151 - acc: 0.9938 - val_loss: 0.4527 - val_acc: 0.9220\n",
      "Epoch 20/100\n",
      "25928/25928 [==============================] - 112s 4ms/step - loss: 0.0141 - acc: 0.9940 - val_loss: 0.5166 - val_acc: 0.9193\n",
      "Epoch 21/100\n",
      "25928/25928 [==============================] - 120s 5ms/step - loss: 0.0147 - acc: 0.9937 - val_loss: 0.4182 - val_acc: 0.9231\n",
      "Epoch 22/100\n",
      "25928/25928 [==============================] - 116s 4ms/step - loss: 0.0121 - acc: 0.9944 - val_loss: 0.5002 - val_acc: 0.9218\n",
      "Epoch 23/100\n",
      "25928/25928 [==============================] - 109s 4ms/step - loss: 0.0135 - acc: 0.9941 - val_loss: 0.4260 - val_acc: 0.9230\n",
      "Epoch 24/100\n",
      "25928/25928 [==============================] - 611s 24ms/step - loss: 0.0124 - acc: 0.9947 - val_loss: 0.4548 - val_acc: 0.9228\n",
      "Epoch 25/100\n",
      "25928/25928 [==============================] - 113s 4ms/step - loss: 0.0122 - acc: 0.9947 - val_loss: 0.4409 - val_acc: 0.9203\n",
      "Epoch 26/100\n",
      "25928/25928 [==============================] - 3091s 119ms/step - loss: 0.0110 - acc: 0.9956 - val_loss: 0.4657 - val_acc: 0.9192\n",
      "Epoch 27/100\n",
      "25928/25928 [==============================] - 112s 4ms/step - loss: 0.0098 - acc: 0.9957 - val_loss: 0.5474 - val_acc: 0.9237\n",
      "Epoch 28/100\n",
      "25928/25928 [==============================] - 106s 4ms/step - loss: 0.0101 - acc: 0.9959 - val_loss: 0.5234 - val_acc: 0.9206\n",
      "Epoch 29/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0100 - acc: 0.9955 - val_loss: 0.4651 - val_acc: 0.9213\n",
      "Epoch 30/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0087 - acc: 0.9965 - val_loss: 0.5418 - val_acc: 0.9214\n",
      "Epoch 31/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0086 - acc: 0.9962 - val_loss: 0.5219 - val_acc: 0.9202\n",
      "Epoch 32/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.5117 - val_acc: 0.9223\n",
      "Epoch 33/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 0.5724 - val_acc: 0.9225\n",
      "Epoch 34/100\n",
      "25928/25928 [==============================] - 109s 4ms/step - loss: 0.0078 - acc: 0.9966 - val_loss: 0.5601 - val_acc: 0.9159\n",
      "Epoch 35/100\n",
      "25928/25928 [==============================] - 117s 5ms/step - loss: 0.0069 - acc: 0.9968 - val_loss: 0.5692 - val_acc: 0.9199\n",
      "Epoch 36/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0064 - acc: 0.9970 - val_loss: 0.6344 - val_acc: 0.9215\n",
      "Epoch 37/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0064 - acc: 0.9970 - val_loss: 0.6446 - val_acc: 0.9182\n",
      "Epoch 38/100\n",
      "25928/25928 [==============================] - 119s 5ms/step - loss: 0.0068 - acc: 0.9971 - val_loss: 0.5904 - val_acc: 0.9221\n",
      "Epoch 39/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0081 - acc: 0.9968 - val_loss: 0.5484 - val_acc: 0.9190\n",
      "Epoch 40/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0065 - acc: 0.9974 - val_loss: 0.5920 - val_acc: 0.9185\n",
      "Epoch 41/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0052 - acc: 0.9979 - val_loss: 0.5683 - val_acc: 0.9163\n",
      "Epoch 42/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0043 - acc: 0.9982 - val_loss: 0.6059 - val_acc: 0.9201\n",
      "Epoch 43/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0065 - acc: 0.9974 - val_loss: 0.5700 - val_acc: 0.9160\n",
      "Epoch 44/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0049 - acc: 0.9978 - val_loss: 0.5706 - val_acc: 0.9202\n",
      "Epoch 45/100\n",
      "25928/25928 [==============================] - 106s 4ms/step - loss: 0.0048 - acc: 0.9977 - val_loss: 0.5750 - val_acc: 0.9170\n",
      "Epoch 46/100\n",
      "25928/25928 [==============================] - 122s 5ms/step - loss: 0.0044 - acc: 0.9981 - val_loss: 0.5750 - val_acc: 0.9184\n",
      "Epoch 47/100\n",
      "25928/25928 [==============================] - 116s 4ms/step - loss: 0.0039 - acc: 0.9982 - val_loss: 0.5847 - val_acc: 0.9186\n",
      "Epoch 48/100\n",
      "25928/25928 [==============================] - 114s 4ms/step - loss: 0.0045 - acc: 0.9980 - val_loss: 0.5808 - val_acc: 0.9158\n",
      "Epoch 49/100\n",
      "25928/25928 [==============================] - 126s 5ms/step - loss: 0.0035 - acc: 0.9983 - val_loss: 0.6196 - val_acc: 0.9186\n",
      "Epoch 50/100\n",
      "25928/25928 [==============================] - 112s 4ms/step - loss: 0.0037 - acc: 0.9984 - val_loss: 0.6186 - val_acc: 0.9180\n",
      "Epoch 51/100\n",
      "25928/25928 [==============================] - 106s 4ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.6259 - val_acc: 0.9185\n",
      "Epoch 52/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 0.6168 - val_acc: 0.9176\n",
      "Epoch 53/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.6480 - val_acc: 0.9189\n",
      "Epoch 54/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.5986 - val_acc: 0.9196\n",
      "Epoch 55/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 0.6441 - val_acc: 0.9221\n",
      "Epoch 56/100\n",
      "25928/25928 [==============================] - 110s 4ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.6353 - val_acc: 0.9183\n",
      "Epoch 57/100\n",
      "25928/25928 [==============================] - 109s 4ms/step - loss: 0.0025 - acc: 0.9989 - val_loss: 0.6517 - val_acc: 0.9195\n",
      "Epoch 58/100\n",
      "25928/25928 [==============================] - 111s 4ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.6037 - val_acc: 0.9179\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0020 - acc: 0.9990 - val_loss: 0.6465 - val_acc: 0.9201\n",
      "Epoch 60/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.6386 - val_acc: 0.9184\n",
      "Epoch 61/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 0.6081 - val_acc: 0.9152\n",
      "Epoch 62/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.6493 - val_acc: 0.9184\n",
      "Epoch 63/100\n",
      "25928/25928 [==============================] - 113s 4ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.6192 - val_acc: 0.9178\n",
      "Epoch 64/100\n",
      "25928/25928 [==============================] - 113s 4ms/step - loss: 0.0016 - acc: 0.9992 - val_loss: 0.6637 - val_acc: 0.9173\n",
      "Epoch 65/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0016 - acc: 0.9993 - val_loss: 0.6826 - val_acc: 0.9190\n",
      "Epoch 66/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.6768 - val_acc: 0.9169\n",
      "Epoch 67/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.6739 - val_acc: 0.9188\n",
      "Epoch 68/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0016 - acc: 0.9992 - val_loss: 0.6890 - val_acc: 0.9193\n",
      "Epoch 69/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.6525 - val_acc: 0.9197\n",
      "Epoch 70/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0016 - acc: 0.9992 - val_loss: 0.6880 - val_acc: 0.9186\n",
      "Epoch 71/100\n",
      "25928/25928 [==============================] - 107s 4ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.6685 - val_acc: 0.9186\n",
      "Epoch 72/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0015 - acc: 0.9993 - val_loss: 0.6272 - val_acc: 0.9190\n",
      "Epoch 73/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0015 - acc: 0.9992 - val_loss: 0.6506 - val_acc: 0.9200\n",
      "Epoch 74/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.6477 - val_acc: 0.9212\n",
      "Epoch 75/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0017 - acc: 0.9993 - val_loss: 0.6382 - val_acc: 0.9190\n",
      "Epoch 76/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0015 - acc: 0.9992 - val_loss: 0.7047 - val_acc: 0.9195\n",
      "Epoch 77/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0015 - acc: 0.9992 - val_loss: 0.6955 - val_acc: 0.9177\n",
      "Epoch 78/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0012 - acc: 0.9993 - val_loss: 0.6917 - val_acc: 0.9187\n",
      "Epoch 79/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0012 - acc: 0.9993 - val_loss: 0.6949 - val_acc: 0.9195\n",
      "Epoch 80/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0024 - acc: 0.9990 - val_loss: 0.7104 - val_acc: 0.9180\n",
      "Epoch 81/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.7165 - val_acc: 0.9179\n",
      "Epoch 82/100\n",
      "25928/25928 [==============================] - 109s 4ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.6861 - val_acc: 0.9192\n",
      "Epoch 83/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0014 - acc: 0.9992 - val_loss: 0.7123 - val_acc: 0.9174\n",
      "Epoch 84/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 8.0897e-04 - acc: 0.9995 - val_loss: 0.6862 - val_acc: 0.9159\n",
      "Epoch 85/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.7062 - val_acc: 0.9202\n",
      "Epoch 86/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 8.4747e-04 - acc: 0.9994 - val_loss: 0.7049 - val_acc: 0.9192\n",
      "Epoch 87/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6765 - val_acc: 0.9185\n",
      "Epoch 88/100\n",
      "25928/25928 [==============================] - 113s 4ms/step - loss: 9.0610e-04 - acc: 0.9995 - val_loss: 0.6970 - val_acc: 0.9189\n",
      "Epoch 89/100\n",
      "25928/25928 [==============================] - 123s 5ms/step - loss: 8.4973e-04 - acc: 0.9994 - val_loss: 0.7297 - val_acc: 0.9206\n",
      "Epoch 90/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 9.7859e-04 - acc: 0.9994 - val_loss: 0.7343 - val_acc: 0.9168\n",
      "Epoch 91/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 0.0011 - acc: 0.9993 - val_loss: 0.7302 - val_acc: 0.9186\n",
      "Epoch 92/100\n",
      "25928/25928 [==============================] - 116s 4ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.7433 - val_acc: 0.9186\n",
      "Epoch 93/100\n",
      "25928/25928 [==============================] - 116s 4ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.7170 - val_acc: 0.9192\n",
      "Epoch 94/100\n",
      "25928/25928 [==============================] - 116s 4ms/step - loss: 0.0015 - acc: 0.9993 - val_loss: 0.7333 - val_acc: 0.9203\n",
      "Epoch 95/100\n",
      "25928/25928 [==============================] - 133s 5ms/step - loss: 8.2449e-04 - acc: 0.9995 - val_loss: 0.7291 - val_acc: 0.9165\n",
      "Epoch 96/100\n",
      "25928/25928 [==============================] - 132s 5ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.7113 - val_acc: 0.9179\n",
      "Epoch 97/100\n",
      "25928/25928 [==============================] - 113s 4ms/step - loss: 7.3701e-04 - acc: 0.9995 - val_loss: 0.7157 - val_acc: 0.9218\n",
      "Epoch 98/100\n",
      "25928/25928 [==============================] - 115s 4ms/step - loss: 7.3967e-04 - acc: 0.9995 - val_loss: 0.7540 - val_acc: 0.9205\n",
      "Epoch 99/100\n",
      "25928/25928 [==============================] - 108s 4ms/step - loss: 6.5301e-04 - acc: 0.9995 - val_loss: 0.7934 - val_acc: 0.9186\n",
      "Epoch 100/100\n",
      "25928/25928 [==============================] - 116s 4ms/step - loss: 0.0010 - acc: 0.9994 - val_loss: 0.7750 - val_acc: 0.9151\n",
      "11112/11112 [==============================] - 4s 329us/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-44d5a887431b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "max_features = 30048\n",
    "maxlen = 97  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "# Building RNN model\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, 128))\n",
    "model_rnn.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_rnn.fit(X_train, y_train,batch_size=batch_size,epochs=100,validation_data=(X_test, y_test))\n",
    "score,acc = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.1574888670357134\n",
      "Test accuracy: 0.9729121670266379\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11112/11112 [==============================] - 2s 181us/step\n",
      "Test accuracy:  0.9036177105831533\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Conv1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "num_features = 30048\n",
    "sequence_length = 97\n",
    "embedding_dimension = 100\n",
    "\n",
    "def model_cnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # use Embedding layer to create vector representation of each word => it is fine-tuned every iteration\n",
    "    model.add(Embedding(input_dim = 900000, output_dim = embedding_dimension, input_length = sequence_length))\n",
    "    model.add(Conv1D(filters = 50, kernel_size = 5, strides = 1, padding = 'valid'))\n",
    "    model.add(MaxPooling1D(2, padding = 'valid'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam , metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_cnn = model_cnn()\n",
    "\n",
    "history = model_cnn.fit(X_train, y_train, batch_size = 50, epochs = 100, validation_split = 0.2, verbose = 0)\n",
    "\n",
    "results = model_cnn.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.7152643856751553\n"
     ]
    }
   ],
   "source": [
    "print('Test score: ', results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25928 samples, validate on 11112 samples\n",
      "Epoch 1/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 0.2676 - acc: 0.8844 - val_loss: 0.1853 - val_acc: 0.9276\n",
      "Epoch 2/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.1151 - acc: 0.9557 - val_loss: 0.1814 - val_acc: 0.9299\n",
      "Epoch 3/100\n",
      "25928/25928 [==============================] - 39s 2ms/step - loss: 0.0641 - acc: 0.9763 - val_loss: 0.2127 - val_acc: 0.9279\n",
      "Epoch 4/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0395 - acc: 0.9851 - val_loss: 0.2862 - val_acc: 0.9261\n",
      "Epoch 5/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0279 - acc: 0.9892 - val_loss: 0.3465 - val_acc: 0.9239\n",
      "Epoch 6/100\n",
      "25928/25928 [==============================] - 44s 2ms/step - loss: 0.0221 - acc: 0.9907 - val_loss: 0.3218 - val_acc: 0.9216\n",
      "Epoch 7/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0192 - acc: 0.9928 - val_loss: 0.4170 - val_acc: 0.9244\n",
      "Epoch 8/100\n",
      "25928/25928 [==============================] - 41s 2ms/step - loss: 0.0188 - acc: 0.9928 - val_loss: 0.3626 - val_acc: 0.9188\n",
      "Epoch 9/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0150 - acc: 0.9939 - val_loss: 0.3904 - val_acc: 0.9194\n",
      "Epoch 10/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0150 - acc: 0.9943 - val_loss: 0.4220 - val_acc: 0.9217\n",
      "Epoch 11/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0139 - acc: 0.9944 - val_loss: 0.4083 - val_acc: 0.9197\n",
      "Epoch 12/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0113 - acc: 0.9950 - val_loss: 0.4587 - val_acc: 0.9126\n",
      "Epoch 13/100\n",
      "25928/25928 [==============================] - 39s 2ms/step - loss: 0.0105 - acc: 0.9954 - val_loss: 0.4928 - val_acc: 0.9202\n",
      "Epoch 14/100\n",
      "25928/25928 [==============================] - 39s 2ms/step - loss: 0.0093 - acc: 0.9961 - val_loss: 0.4488 - val_acc: 0.9174\n",
      "Epoch 15/100\n",
      "25928/25928 [==============================] - 39s 2ms/step - loss: 0.0102 - acc: 0.9960 - val_loss: 0.4251 - val_acc: 0.9162\n",
      "Epoch 16/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0096 - acc: 0.9964 - val_loss: 0.4642 - val_acc: 0.9178\n",
      "Epoch 17/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0090 - acc: 0.9964 - val_loss: 0.4986 - val_acc: 0.9214\n",
      "Epoch 18/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0072 - acc: 0.9970 - val_loss: 0.5490 - val_acc: 0.9205\n",
      "Epoch 19/100\n",
      "25928/25928 [==============================] - 42s 2ms/step - loss: 0.0077 - acc: 0.9968 - val_loss: 0.4950 - val_acc: 0.9158\n",
      "Epoch 20/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0077 - acc: 0.9965 - val_loss: 0.5033 - val_acc: 0.9169\n",
      "Epoch 21/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 0.0076 - acc: 0.9969 - val_loss: 0.5181 - val_acc: 0.9200\n",
      "Epoch 22/100\n",
      "25928/25928 [==============================] - 42s 2ms/step - loss: 0.0067 - acc: 0.9970 - val_loss: 0.5206 - val_acc: 0.9153\n",
      "Epoch 23/100\n",
      "25928/25928 [==============================] - 45s 2ms/step - loss: 0.0060 - acc: 0.9973 - val_loss: 0.5848 - val_acc: 0.9149\n",
      "Epoch 24/100\n",
      "25928/25928 [==============================] - 47s 2ms/step - loss: 0.0059 - acc: 0.9975 - val_loss: 0.5448 - val_acc: 0.9160\n",
      "Epoch 25/100\n",
      "25928/25928 [==============================] - 46s 2ms/step - loss: 0.0047 - acc: 0.9980 - val_loss: 0.6017 - val_acc: 0.9166\n",
      "Epoch 26/100\n",
      "25928/25928 [==============================] - 48s 2ms/step - loss: 0.0048 - acc: 0.9978 - val_loss: 0.6056 - val_acc: 0.9177\n",
      "Epoch 27/100\n",
      "25928/25928 [==============================] - 49s 2ms/step - loss: 0.0052 - acc: 0.9978 - val_loss: 0.5630 - val_acc: 0.9150\n",
      "Epoch 28/100\n",
      "25928/25928 [==============================] - 49s 2ms/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.5357 - val_acc: 0.9169\n",
      "Epoch 29/100\n",
      "25928/25928 [==============================] - 49s 2ms/step - loss: 0.0039 - acc: 0.9983 - val_loss: 0.6250 - val_acc: 0.9183\n",
      "Epoch 30/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0037 - acc: 0.9984 - val_loss: 0.5986 - val_acc: 0.9195\n",
      "Epoch 31/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.6346 - val_acc: 0.9177\n",
      "Epoch 32/100\n",
      "25928/25928 [==============================] - 49s 2ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.6206 - val_acc: 0.9167\n",
      "Epoch 33/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 0.5563 - val_acc: 0.9151\n",
      "Epoch 34/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.5654 - val_acc: 0.9143\n",
      "Epoch 35/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.6245 - val_acc: 0.9171\n",
      "Epoch 36/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0053 - acc: 0.9979 - val_loss: 0.6057 - val_acc: 0.9180\n",
      "Epoch 37/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.5435 - val_acc: 0.9181\n",
      "Epoch 38/100\n",
      "25928/25928 [==============================] - 53s 2ms/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.5262 - val_acc: 0.9168\n",
      "Epoch 39/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.5343 - val_acc: 0.9193\n",
      "Epoch 40/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.6138 - val_acc: 0.9200\n",
      "Epoch 41/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.5194 - val_acc: 0.9172\n",
      "Epoch 42/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.5874 - val_acc: 0.9212\n",
      "Epoch 43/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0026 - acc: 0.9989 - val_loss: 0.5539 - val_acc: 0.9161\n",
      "Epoch 44/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0027 - acc: 0.9988 - val_loss: 0.5760 - val_acc: 0.9163\n",
      "Epoch 45/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.6491 - val_acc: 0.9095\n",
      "Epoch 46/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0020 - acc: 0.9991 - val_loss: 0.6305 - val_acc: 0.9175\n",
      "Epoch 47/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.6679 - val_acc: 0.9172\n",
      "Epoch 48/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.5874 - val_acc: 0.9155\n",
      "Epoch 49/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0022 - acc: 0.9991 - val_loss: 0.6008 - val_acc: 0.9204\n",
      "Epoch 50/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.6496 - val_acc: 0.9179\n",
      "Epoch 51/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.5980 - val_acc: 0.9159\n",
      "Epoch 52/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0024 - acc: 0.9990 - val_loss: 0.5428 - val_acc: 0.9194\n",
      "Epoch 53/100\n",
      "25928/25928 [==============================] - 48s 2ms/step - loss: 0.0024 - acc: 0.9990 - val_loss: 0.5620 - val_acc: 0.9153\n",
      "Epoch 54/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.5833 - val_acc: 0.9159\n",
      "Epoch 55/100\n",
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.5608 - val_acc: 0.9133\n",
      "Epoch 56/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 8.4395e-04 - acc: 0.9996 - val_loss: 0.5954 - val_acc: 0.9152\n",
      "Epoch 57/100\n",
      "25928/25928 [==============================] - 48s 2ms/step - loss: 9.8875e-04 - acc: 0.9995 - val_loss: 0.6325 - val_acc: 0.9160\n",
      "Epoch 58/100\n",
      "25928/25928 [==============================] - 49s 2ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.5833 - val_acc: 0.9165\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25928/25928 [==============================] - 50s 2ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.5835 - val_acc: 0.9146\n",
      "Epoch 60/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0024 - acc: 0.9990 - val_loss: 0.5311 - val_acc: 0.9128\n",
      "Epoch 61/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.6213 - val_acc: 0.9151\n",
      "Epoch 62/100\n",
      "25928/25928 [==============================] - 53s 2ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.6082 - val_acc: 0.9062\n",
      "Epoch 63/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 9.1453e-04 - acc: 0.9997 - val_loss: 0.6305 - val_acc: 0.9152\n",
      "Epoch 64/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.6094 - val_acc: 0.9159\n",
      "Epoch 65/100\n",
      "25928/25928 [==============================] - 59s 2ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 0.6320 - val_acc: 0.9125\n",
      "Epoch 66/100\n",
      "25928/25928 [==============================] - 53s 2ms/step - loss: 0.0014 - acc: 0.9993 - val_loss: 0.6004 - val_acc: 0.9105\n",
      "Epoch 67/100\n",
      "25928/25928 [==============================] - 54s 2ms/step - loss: 8.7691e-04 - acc: 0.9996 - val_loss: 0.6406 - val_acc: 0.9123\n",
      "Epoch 68/100\n",
      "25928/25928 [==============================] - 54s 2ms/step - loss: 7.1031e-04 - acc: 0.9996 - val_loss: 0.6842 - val_acc: 0.9142\n",
      "Epoch 69/100\n",
      "25928/25928 [==============================] - 53s 2ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.5763 - val_acc: 0.9116\n",
      "Epoch 70/100\n",
      "25928/25928 [==============================] - 54s 2ms/step - loss: 9.7957e-04 - acc: 0.9995 - val_loss: 0.6104 - val_acc: 0.9114\n",
      "Epoch 71/100\n",
      "25928/25928 [==============================] - 58s 2ms/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.6261 - val_acc: 0.9121\n",
      "Epoch 72/100\n",
      "25928/25928 [==============================] - 58s 2ms/step - loss: 0.0016 - acc: 0.9993 - val_loss: 0.6153 - val_acc: 0.9144\n",
      "Epoch 73/100\n",
      "25928/25928 [==============================] - 54s 2ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.6499 - val_acc: 0.9141\n",
      "Epoch 74/100\n",
      "25928/25928 [==============================] - 53s 2ms/step - loss: 0.0011 - acc: 0.9994 - val_loss: 0.6065 - val_acc: 0.9117\n",
      "Epoch 75/100\n",
      "25928/25928 [==============================] - 53s 2ms/step - loss: 9.7872e-04 - acc: 0.9995 - val_loss: 0.5993 - val_acc: 0.9123\n",
      "Epoch 76/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 6.7539e-04 - acc: 0.9996 - val_loss: 0.6577 - val_acc: 0.9126\n",
      "Epoch 77/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 6.2053e-04 - acc: 0.9996 - val_loss: 0.6672 - val_acc: 0.9143\n",
      "Epoch 78/100\n",
      "25928/25928 [==============================] - 51s 2ms/step - loss: 6.1342e-04 - acc: 0.9996 - val_loss: 0.6937 - val_acc: 0.9143\n",
      "Epoch 79/100\n",
      "25928/25928 [==============================] - 52s 2ms/step - loss: 9.8179e-04 - acc: 0.9995 - val_loss: 0.6956 - val_acc: 0.9120\n",
      "Epoch 80/100\n",
      "25928/25928 [==============================] - 45s 2ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.6490 - val_acc: 0.9150\n",
      "Epoch 81/100\n",
      "25928/25928 [==============================] - 42s 2ms/step - loss: 0.0011 - acc: 0.9994 - val_loss: 0.6741 - val_acc: 0.9141\n",
      "Epoch 82/100\n",
      "25928/25928 [==============================] - 42s 2ms/step - loss: 9.7980e-04 - acc: 0.9994 - val_loss: 0.6534 - val_acc: 0.9156\n",
      "Epoch 83/100\n",
      "25928/25928 [==============================] - 42s 2ms/step - loss: 0.0012 - acc: 0.9994 - val_loss: 0.6671 - val_acc: 0.9184\n",
      "Epoch 84/100\n",
      "25928/25928 [==============================] - 41s 2ms/step - loss: 7.9836e-04 - acc: 0.9996 - val_loss: 0.6566 - val_acc: 0.9165\n",
      "Epoch 85/100\n",
      "25928/25928 [==============================] - 42s 2ms/step - loss: 8.1738e-04 - acc: 0.9994 - val_loss: 0.6700 - val_acc: 0.9159\n",
      "Epoch 86/100\n",
      "25928/25928 [==============================] - 43s 2ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.6725 - val_acc: 0.9137\n",
      "Epoch 87/100\n",
      "25928/25928 [==============================] - 43s 2ms/step - loss: 0.0013 - acc: 0.9993 - val_loss: 0.6455 - val_acc: 0.9169\n",
      "Epoch 88/100\n",
      "25928/25928 [==============================] - 47s 2ms/step - loss: 8.6931e-04 - acc: 0.9995 - val_loss: 0.6111 - val_acc: 0.9122\n",
      "Epoch 89/100\n",
      "25928/25928 [==============================] - 42s 2ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.5353 - val_acc: 0.9159\n",
      "Epoch 90/100\n",
      "25928/25928 [==============================] - 44s 2ms/step - loss: 8.0768e-04 - acc: 0.9994 - val_loss: 0.5894 - val_acc: 0.9163\n",
      "Epoch 91/100\n",
      "25928/25928 [==============================] - 41s 2ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.5478 - val_acc: 0.9137\n",
      "Epoch 92/100\n",
      "25928/25928 [==============================] - 43s 2ms/step - loss: 8.8927e-04 - acc: 0.9995 - val_loss: 0.5885 - val_acc: 0.9163\n",
      "Epoch 93/100\n",
      "25928/25928 [==============================] - 43s 2ms/step - loss: 0.0011 - acc: 0.9994 - val_loss: 0.5841 - val_acc: 0.9113\n",
      "Epoch 94/100\n",
      "25928/25928 [==============================] - 41s 2ms/step - loss: 0.0014 - acc: 0.9993 - val_loss: 0.6036 - val_acc: 0.9152\n",
      "Epoch 95/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 6.4517e-04 - acc: 0.9996 - val_loss: 0.6214 - val_acc: 0.9159\n",
      "Epoch 96/100\n",
      "25928/25928 [==============================] - 41s 2ms/step - loss: 6.4925e-04 - acc: 0.9995 - val_loss: 0.6438 - val_acc: 0.9150\n",
      "Epoch 97/100\n",
      "25928/25928 [==============================] - 40s 2ms/step - loss: 5.8073e-04 - acc: 0.9995 - val_loss: 0.6747 - val_acc: 0.9155\n",
      "Epoch 98/100\n",
      "25928/25928 [==============================] - 47s 2ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.5762 - val_acc: 0.9155\n",
      "Epoch 99/100\n",
      "25928/25928 [==============================] - 47s 2ms/step - loss: 0.0014 - acc: 0.9993 - val_loss: 0.5794 - val_acc: 0.9187\n",
      "Epoch 100/100\n",
      "25928/25928 [==============================] - 47s 2ms/step - loss: 8.0431e-04 - acc: 0.9995 - val_loss: 0.6343 - val_acc: 0.9147\n",
      "11112/11112 [==============================] - 3s 289us/step\n",
      "Test score: 0.6342775312188352\n",
      "Test accuracy: 0.9146868214708511\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "maxfeatures = 30048\n",
    "max_len = 97\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxfeatures, embedding_size, input_length=max_len))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.6342775312188352\n",
      "Test accuracy: 0.9146868214708511\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
