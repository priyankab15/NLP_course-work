{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Movie Classification, the sequel\n",
    "![](https://images-na.ssl-images-amazon.com/images/S/sgp-catalog-images/region_US/paramount-01376-Full-Image_GalleryBackground-en-US-1484000188762._RI_SX940_.jpg)\n",
    "\n",
    "\n",
    "#### In this assignment, we will learn a little more about word2vec and then use the resulting vectors to make some predictions.\n",
    "\n",
    "We will be working with a movie synopsis dataset, found here: http://www.cs.cmu.edu/~ark/personas/\n",
    "\n",
    "The overall goal should sound a little familiar - based on the movie synopses, we will classify movie genre. Some of your favorites should be in this dataset, and hopefully, based on the genre specific terminology of the movie synopses, we will be able to figure out which movies are which type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: clean your dataset!\n",
    "\n",
    "For your input data:\n",
    "\n",
    "1. Find the top 10 movie genres\n",
    "2. Remove any synopses that don't fit into these genres\n",
    "3. Take the top 10,000 reviews in terms of \"Movie box office revenue\"\n",
    "\n",
    "Congrats, you've got a dataset! For each movie, some of them may have multiple classifications. To deal with this, you'll have to look at the Reuters dataset classification code that we used previously and possibly this example: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "\n",
    "We want to use categorical cross-entropy as our loss function (or a one vs. all classifier in the case of SVM) because our data will potentially have multiple classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42424\n",
      "['Drama', 'Comedy', 'Romance Film', 'Thriller', 'Action', 'Action/Adventure', 'Crime Fiction', 'Adventure', 'Indie', 'Romantic comedy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "os.chdir(\"/home/priya/Downloads/MovieSummaries\")\n",
    "\n",
    "\n",
    "df= pd.read_csv(\"movie.metadata.tsv\", sep=\"\\t\", header=0)\n",
    "df.columns=[\"Wikipedia movie ID\", \"Freebase movie ID\",\"Movie name\", \"Movie release date\", \n",
    "            \"Movie box office revenue\", \"Movie runtime\", \"Movie languages\", \"Movie countries\", \"Movie genres\"]\n",
    "\n",
    "#Remove null values from Movie box office revenue\n",
    "df = df[df['Movie box office revenue'].notnull()] \n",
    " \n",
    "y = df['Movie genres']\n",
    "\n",
    "y_new =[]\n",
    "for index,item in enumerate(y):\n",
    "    m = eval(item)\n",
    "    y_new.append(list(m.values()))\n",
    "\n",
    "flat_list = [item for sublist in y_new for item in sublist]\n",
    "    \n",
    "    \n",
    "print(len(flat_list))\n",
    "\n",
    "flat_list_10 = Counter(flat_list)\n",
    "\n",
    "top_10_movie_genres = [item[0] for item in flat_list_10.most_common(10)]\n",
    "print(top_10_movie_genres)\n",
    "keep_genre = []\n",
    "\n",
    "for item in y_new:\n",
    "    genre = list(set(item).intersection(set(top_10_movie_genres)))\n",
    "    if len(genre)>0:\n",
    "        keep_genre.append(genre[0])\n",
    "    else:\n",
    "        keep_genre.append(np.nan)\n",
    "        \n",
    "        \n",
    "df[\"Movie genres\"] = keep_genre\n",
    "\n",
    "df = df[df['Movie genres'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7195, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"plot_summaries.txt\", 'r',encoding='utf-8') as FR:\n",
    "       plots = FR.readlines()\n",
    "\n",
    "plots = {x.split('\\t')[0]:x.split('\\t')[1] for x in plots}\n",
    "\n",
    "df['plots'] = [plots[str(key)] if str(key) in plots else np.nan for key in df[\"Wikipedia movie ID\"]]\n",
    "\n",
    "df = df[df['plots'].notnull()]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shlykov, a hard-working taxi driver and Lyosha, a saxophonist, develop a bizarre love-hate relationship, and despite their prejudices, realize they aren't so different after all.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item,values in plots.items():\n",
    "    print(values)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def getsynopses(series):\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(series.lower())\n",
    "    processed_word_list = [i for i in tokens if i not in stop and len(i)>2]\n",
    "    return processed_word_list\n",
    "     \n",
    "df[['plots']] = df[['plots']].applymap(lambda m:getsynopses(m))\n",
    "X = list(df['plots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model = gensim.models.Word2Vec(X, iter=10, min_count=10, size=200, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priya/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "vocab = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "vecArray = []\n",
    "for words in X:\n",
    "    w2v_array = []\n",
    "    for w in words:\n",
    "        if w in vocab:\n",
    "            w2v_array.append( vocab[w] )\n",
    "    vecArray.append(np.mean(w2v_array,axis=0))\n",
    "vecArray =np.array(vecArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.27888116240501404, -0.38078123331069946, 0....</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.06917796283960342, -0.15979351103305817, 0....</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.13169050216674805, 0.05687893182039261, 0.1...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.03149157762527466, -0.06241234019398689, 0...</td>\n",
       "      <td>Action/Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.04280886799097061, -0.07591108232736588, 0...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X                 y\n",
       "0  [0.27888116240501404, -0.38078123331069946, 0....             Drama\n",
       "1  [0.06917796283960342, -0.15979351103305817, 0....             Drama\n",
       "2  [0.13169050216674805, 0.05687893182039261, 0.1...             Drama\n",
       "3  [-0.03149157762527466, -0.06241234019398689, 0...  Action/Adventure\n",
       "4  [-0.04280886799097061, -0.07591108232736588, 0...             Drama"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecArray_series = pd.Series(vecArray.tolist())\n",
    "\n",
    "genre = list(df['Movie genres'])\n",
    "test= list(df['plots'])\n",
    "df_data = pd.DataFrame({\"X\":vecArray_series,\"y\":genre})\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split the data\n",
    "\n",
    "Make a dataset of 70% train and 30% test. Sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data.X,df_data.y,test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "def getSeries(series):\n",
    "        val_list = []\n",
    "        val_list.append(series)\n",
    "        return val_list\n",
    "        \n",
    "\n",
    "y_train = y_train.apply(lambda m:getSeries(m))\n",
    "print(type(y_train))\n",
    "y_test = y_test.apply(lambda m:getSeries(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_movies_list = [['Drama'],\n",
    " ['Comedy'],\n",
    " ['Romance Film'],\n",
    " ['Thriller'],\n",
    " ['Action'],\n",
    " ['Action/Adventure'],\n",
    " ['Crime Fiction'],\n",
    " ['Adventure'],\n",
    " ['Indie'],\n",
    " ['Romantic comedy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Build a model using ONLY word2vec\n",
    "\n",
    "Woah what? I don't think that's recommended...\n",
    "\n",
    "In fact it's a commonly accepted practice. What you will want to do is average the word vectors that will be input for a given synopsis (https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and then input that averaged vector as your feature space into a model. For this example, use a Support Vector Machine classifier. For your first time doing this, train a model in Gensim and use the output vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "binarizer = MultiLabelBinarizer()\n",
    "binarizer.fit(top_movies_list)\n",
    "\n",
    "train_label= binarizer.fit_transform(y_train)\n",
    "test_label = binarizer.fit_transform(y_test)\n",
    "y_train = train_label\n",
    "y_test = test_label\n",
    "\n",
    "train_list  = []\n",
    "for val in X_train:\n",
    "    train_list.append(np.array(val))\n",
    "\n",
    "final_train_list = np.array(train_list)\n",
    "print(type(final_train_list))\n",
    "#print(final_train_list)\n",
    "test_list  = []\n",
    "for val in X_test:\n",
    "    test_list.append(np.array(val))\n",
    "\n",
    "final_test_list = np.array(test_list)\n",
    "#print(final_test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "model = classifier.fit(final_train_list, y_train)\n",
    "\n",
    "\n",
    "predictions = model.predict(final_test_list)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='macro')\n",
    "    recall = recall_score(test_labels, predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels,predictions)\n",
    "\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, Accuracy: {:.4f}\".format(precision, recall,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2972, Recall: 0.1186, Accuracy: 0.4553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Do the same thing but with pretrained embeddings\n",
    "\n",
    "Now pull down the Google News word embeddings and do the same thing. Compare the results. Why was one better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "#model2 = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42424\n",
      "['Drama', 'Comedy', 'Romance Film', 'Thriller', 'Action', 'Action/Adventure', 'Crime Fiction', 'Adventure', 'Indie', 'Romantic comedy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7195, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "df= pd.read_csv(\"movie.metadata.tsv\", sep=\"\\t\", header=0)\n",
    "df.columns=[\"Wikipedia movie ID\", \"Freebase movie ID\",\"Movie name\", \"Movie release date\", \n",
    "            \"Movie box office revenue\", \"Movie runtime\", \"Movie languages\", \"Movie countries\", \"Movie genres\"]\n",
    "\n",
    "#Remove null values from Movie box office revenue\n",
    "df = df[df['Movie box office revenue'].notnull()]\n",
    "y = df['Movie genres']\n",
    "\n",
    "y_new =[]\n",
    "for index,item in enumerate(y):\n",
    "    m = eval(item)\n",
    "    y_new.append(list(m.values()))\n",
    "\n",
    "flat_list = [item for sublist in y_new for item in sublist]\n",
    "    \n",
    "    \n",
    "print(len(flat_list))\n",
    "flat_list_10 = Counter(flat_list)\n",
    "\n",
    "top_10_movie_genres = [item[0] for item in flat_list_10.most_common(10)]\n",
    "print(top_10_movie_genres)\n",
    "keep_genre = []\n",
    "\n",
    "for item in y_new:\n",
    "    genre = list(set(item).intersection(set(top_10_movie_genres)))\n",
    "    if len(genre)>0:\n",
    "        keep_genre.append(genre[0])\n",
    "    else:\n",
    "        keep_genre.append(np.nan)\n",
    "        \n",
    "        \n",
    "df[\"Movie genres\"] = keep_genre\n",
    "\n",
    "df = df[df['Movie genres'].notnull()]\n",
    "\n",
    "with open(\"plot_summaries.txt\", 'r',encoding='utf-8') as FR:\n",
    "       plots = FR.readlines()\n",
    "\n",
    "plots = {x.split('\\t')[0]:x.split('\\t')[1] for x in plots}\n",
    "\n",
    "df['plots'] = [plots[str(key)] if str(key) in plots else np.nan for key in df[\"Wikipedia movie ID\"]]\n",
    "df = df[df['plots'].notnull()]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def getsynopses(series):\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(series.lower())\n",
    "    processed_word_list = [i for i in tokens if i not in stop and len(i)>2]\n",
    "    return processed_word_list\n",
    "     \n",
    "\n",
    "\n",
    "df[['plots']] = df[['plots']].applymap(lambda m:getsynopses(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = list(df['plots'])\n",
    "vocab2_words = model2.vocab.keys()\n",
    "vocab2 = {}\n",
    "for word in vocab2_words:\n",
    "    vocab2[word] = model2[word]\n",
    "vecArray = []\n",
    "for words in X:\n",
    "    w2v_array = []\n",
    "    for w in words:\n",
    "        if w in vocab2:\n",
    "            w2v_array.append( vocab2[w] )\n",
    "    vecArray.append(np.mean(w2v_array,axis=0))\n",
    "vecArray =np.array(vecArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecArray_series = pd.Series(vecArray.tolist())\n",
    "genre = list(df['Movie genres'])\n",
    "df_data = pd.DataFrame({\"X\":vecArray_series,\"y\":genre})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data.X,df_data.y,\n",
    "                                                    test_size=0.3,stratify=df_data.y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSeries(series):\n",
    "        val_list = []\n",
    "        val_list.append(series)\n",
    "        return val_list\n",
    "        \n",
    "              \n",
    "y_train = y_train.apply(lambda m:getSeries(m))\n",
    "y_test = y_test.apply(lambda m:getSeries(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_movies_list = [['Drama'],\n",
    " ['Comedy'],\n",
    " ['Romance Film'],\n",
    " ['Thriller'],\n",
    " ['Action'],\n",
    " ['Action/Adventure'],\n",
    " ['Crime Fiction'],\n",
    " ['Adventure'],\n",
    " ['Indie'],\n",
    " ['Romantic comedy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "binarizer = MultiLabelBinarizer()\n",
    "binarizer.fit(top_movies_list)\n",
    "\n",
    "\n",
    "train_label = binarizer.fit_transform(y_train)\n",
    "test_label = binarizer.fit_transform(y_test)\n",
    "\n",
    "y_train = train_label\n",
    "y_test = test_label\n",
    "\n",
    "train_list  = []\n",
    "\n",
    "for val in X_train:    \n",
    "    train_list.append(np.array(val))\n",
    "\n",
    "final_train_list = np.array(train_list)\n",
    "\n",
    "test_list  = []\n",
    "for val in X_test:\n",
    "    test_list.append(np.array(val))\n",
    "\n",
    "final_test_list = np.array(test_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "model = classifier.fit(final_train_list, y_train)\n",
    "\n",
    "\n",
    "predictions = model.predict(final_test_list)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='macro')\n",
    "    recall = recall_score(test_labels, predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels,predictions)\n",
    "\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, Accuracy: {:.4f}\".format(precision, recall,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2946, Recall: 0.1053, Accuracy: 0.4423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Build a neural net model using word2vec embeddings (both pretrained and within an Embedding layer from Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model_textual = Sequential([\n",
    "    Dense(300, input_shape=(5036,1)),\n",
    "    Activation('relu'),\n",
    "    Dense(24),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model_textual.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_textual.fit(X_train, y_train, epochs=10, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416/1416 [==============================] - 0s 118us/step\n"
     ]
    }
   ],
   "source": [
    "score1 = model_textual.evaluate(X_test, Y_test, batch_size=249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 73.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model_textual.metrics_names[1], score1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Change the architecture of your model and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(300,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(24))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27297eec710>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10000, batch_size=500,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416/1416 [==============================] - 0s 23us/step\n"
     ]
    }
   ],
   "source": [
    "score = model_textual.evaluate(X_test, Y_test, batch_size=249)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: For each model, do an error evaluation\n",
    "\n",
    "You now have a bunch of classifiers. For each classifier, pick 2 good classifications and 2 bad classifications. Print the expected and predicted label, and also print the movie synopsis. From these results, can you spot some systematic errors from your models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 74.23%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model_textual.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_preds=model_textual.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
