{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Named Entity Recognition\n",
    "\n",
    "In this assignment, we are going to build a Named Entity Recognition model. With this model, we will also tag new data.\n",
    "\n",
    "More on Named Entity Recognition:\n",
    "\n",
    "https://blog.paralleldots.com/data-science/named-entity-recognition-milestone-models-papers-and-technologies/\n",
    "\n",
    "https://blog.paralleldots.com/product/applications-named-entity-recognition-api/\n",
    "\n",
    "### Steps:\n",
    "\n",
    "**1. Import the data**\n",
    "\n",
    "**2. Build the model**\n",
    "\n",
    "**3. Pick a dataset to run the model on**\n",
    "\n",
    "**4. Build a function to load new data and print the tags**\n",
    "\n",
    "Your web application will load small sections of text (such as tweets or headlines) and from that, you will tag the text based on the presence of named entities.\n",
    "\n",
    "*What you will be graded on:*\n",
    "\n",
    "1. Ability to build a model on word and tag data\n",
    "\n",
    "2. Ability to use the model to predict on new data and display that prediction\n",
    "\n",
    "*The model will be based on:*\n",
    "1. Embeddings from words\n",
    "2. Embeddings from tag inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the data\n",
    "\n",
    "Below is some code to get you started. As in the part of speech tagging example, you will have to write code to:\n",
    "\n",
    "0. Split your data into a train/test set (Do a 80/20 or 90/10 split since we'll be later applying this model to an entirely separate set of data)\n",
    "1. Find the set of all words\n",
    "2. Find the set of all tags\n",
    "3. **Create a function called ent_tagger** that will turn a sentence into this output for model building :\n",
    "``` [('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have',  'O'), ('marched',  'O'), ('through',  'O'), ('London', 'B-geo'), ('to',  'O'), ('protest',  'O'), ('the',  'O'), ('war',  'O'), ('in',  'O'), ('Iraq',  'B-geo'), ('and', 'O'), ('demand',  'O'), ('the',  'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops',  'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n",
    "```\n",
    "4. Make a dictionary of words to index and entity tag to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentence #           Word  POS    Tag\n",
      "0  Sentence: 1      Thousands  NNS      O\n",
      "1  Sentence: 1             of   IN      O\n",
      "2  Sentence: 1  demonstrators  NNS      O\n",
      "3  Sentence: 1           have  VBP      O\n",
      "4  Sentence: 1        marched  VBN      O\n",
      "5  Sentence: 1        through   IN      O\n",
      "6  Sentence: 1         London  NNP  B-geo\n",
      "7  Sentence: 1             to   TO      O\n",
      "8  Sentence: 1        protest   VB      O\n",
      "9  Sentence: 1            the   DT      O\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.chdir(\"/Users/priya/Downloads\")\n",
    "\n",
    "### NER DATASET IS FOUND IN THE COURSE REPO\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "print(data.head(10))\n",
    "X= data.iloc[:,0:3]\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35178\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "print(n_words)\n",
    "\n",
    "tags= list(set(data[\"Tag\"].values))\n",
    "n_tags= len(tags)\n",
    "print(n_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('London', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('Iraq', 'B-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n",
      "[('Iranian', 'B-gpe'), ('officials', 'O'), ('say', 'O'), ('they', 'O'), ('expect', 'O'), ('to', 'O'), ('get', 'O'), ('access', 'O'), ('to', 'O'), ('sealed', 'O'), ('sensitive', 'O'), ('parts', 'O'), ('of', 'O'), ('the', 'O'), ('plant', 'O'), ('Wednesday', 'B-tim'), (',', 'O'), ('after', 'O'), ('an', 'O'), ('IAEA', 'B-org'), ('surveillance', 'O'), ('system', 'O'), ('begins', 'O'), ('functioning', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "                                                           \n",
    "                                                           \n",
    "    def ent_tagger(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.ent_tagger()\n",
    "print(sent)\n",
    "sentences= getter.sentences\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Formatting the data\n",
    "Data will need to be\n",
    "\n",
    "1. Indexed\n",
    "2. Limited by vocabulary (ie replace tokens with UNKNOWN if they are too rare, come up with a reasonable limit based on your survey of the data and also model performance)\n",
    "3. Padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>26354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>5485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>6</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>23213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>9</td>\n",
       "      <td>52573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag  index   freq\n",
       "0  Sentence: 1      Thousands  NNS      O      0    114\n",
       "1  Sentence: 1             of   IN      O      1  26354\n",
       "2  Sentence: 1  demonstrators  NNS      O      2    110\n",
       "3  Sentence: 1           have  VBP      O      3   5485\n",
       "4  Sentence: 1        marched  VBN      O      4     65\n",
       "5  Sentence: 1        through   IN      O      5    515\n",
       "6  Sentence: 1         London  NNP  B-geo      6    261\n",
       "7  Sentence: 1             to   TO      O      7  23213\n",
       "8  Sentence: 1        protest   VB      O      8    237\n",
       "9  Sentence: 1            the   DT      O      9  52573"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic= data[\"Word\"].unique()\n",
    "indexed={}\n",
    "l=len(dic)\n",
    "for w in range(l):\n",
    "    indexed[dic[w]]=w\n",
    "# print(indexed)    \n",
    "uniqueindex=[]\n",
    "for w in range(0,len(data)):\n",
    "    key= data[\"Word\"][w]\n",
    "    value= indexed[key]\n",
    "    uniqueindex.append(value)\n",
    "\n",
    "data[\"index\"]=uniqueindex\n",
    "data['freq'] = data.groupby('Word')['Word'].transform('count')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len= 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value= tag2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Build the model\n",
    "\n",
    "Here we will build a Bidirectional LSTM-CRF model using the `Bidirectional` function from Keras and `CRF` function from Keras-contrib\n",
    "\n",
    "**Documentation and source code:**\n",
    "\n",
    "https://keras.io/layers/wrappers/#bidirectional\n",
    "\n",
    "https://github.com/keras-team/keras-contrib\n",
    "\n",
    "Fit your model with a validation split of 0.1, feel free to use as many epochs as you like. Base your predictions both from the input words **and** the tags from previous words like in the POS example.\n",
    "\n",
    "After building your model, grade your performance on your test set, both by comparing your predicted output to the actual (*at least 3 examples*) and calculate the averaged precision and recall for your tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words + 1, output_dim=20,\n",
    "                  input_length=max_len, mask_zero=True)(input)  # 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(n_tags)  # CRF layer\n",
    "out = crf(model)  # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30213 samples, validate on 3358 samples\n",
      "Epoch 1/3\n",
      "30213/30213 [==============================] - 314s 10ms/step - loss: 8.9125 - val_loss: 8.7066\n",
      "Epoch 2/3\n",
      "30213/30213 [==============================] - 516s 17ms/step - loss: 8.7467 - val_loss: 8.6712\n",
      "Epoch 3/3\n",
      "30213/30213 [==============================] - 309s 10ms/step - loss: 8.7265 - val_loss: 8.6548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a494d39e8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input, out)\n",
    "model.compile(optimizer='rmsprop', loss=crf.loss_function)\n",
    "model.fit(X_train, np.array(y_train), epochs=3, batch_size=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Actual Pred\n",
      "criminalizes        : O      O\n",
      "symptom             : O      O\n",
      "renaming            : O      O\n",
      "Teams               : O      O\n",
      "Yekiti              : O      O\n",
      "arena               : O      O\n",
      "Suu                 : O      O\n",
      "chancellor          : O      O\n",
      "plundered           : O      O\n",
      "tours               : O      O\n",
      "hijackers           : O      O\n",
      "Schatten            : O      O\n",
      "Arafat              : O      O\n",
      "J.S.                : O      O\n",
      "Gouled              : O      O\n",
      "Yekiti              : O      O\n",
      "economic            : B-gpe  B-geo\n",
      "Self-Defense        : O      O\n",
      "facets              : O      O\n",
      "Hobart              : O      O\n",
      "tours               : O      O\n",
      "quelling            : O      O\n",
      "Schatten            : O      O\n",
      "Arafat              : O      O\n",
      "3,700               : O      O\n",
      "holiday             : O      O\n",
      "Paulos              : O      O\n",
      "tours               : O      O\n",
      "crewmembers         : O      O\n",
      "determining         : O      O\n",
      "DeLay               : O      O\n"
     ]
    }
   ],
   "source": [
    "i=100\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "Actual = np.argmax(y_test[i], -1)\n",
    "print(\"{:20}{:6} {:}\".format(\"Word\", \"Actual\", \"Pred\"))\n",
    "pred_tags=[]\n",
    "actual_tags=[]\n",
    "for w,t, pred in zip(X_test[i],Actual,p[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:20}: {:6} {:}\".format(words[w-1], tags[t], tags[pred]))\n",
    "        pred_tags.append(tags[pred])\n",
    "        actual_tags.append(tags[t])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.968\n",
      "PRECISION: 0.968\n",
      "RECALL: 0.968\n",
      "F1: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/priya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/priya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/priya/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(actual_tags,pred_tags)\n",
    "precision = precision_score(actual_tags,pred_tags, average='weighted')\n",
    "recall = recall_score(actual_tags,pred_tags, average='weighted')\n",
    "f1 = f1_score(actual_tags,pred_tags, average='weighted')\n",
    "    \n",
    "print(\"ACCURACY: {:.3f}\".format(accuracy))\n",
    "print(\"PRECISION: {:.3f}\".format(precision))\n",
    "print(\"RECALL: {:.3f}\".format(recall))\n",
    "print(\"F1: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Pick a dataset\n",
    "\n",
    "Pick a dataset that has short text, similar to the sentences you just tagged. Headlines and tweets are good choices.\n",
    "\n",
    "https://www.kaggle.com/datasets?sortBy=relevance&group=public&search=news&page=1&pageSize=20&size=all&filetype=all&license=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"abcnews-date-text.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1103665/1103665 [00:11<00:00, 93972.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from string import punctuation, ascii_lowercase\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setup tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def text_to_wordlist(text, lower=False):\n",
    "    \n",
    "    # Tokenize\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "   \n",
    "    # Return a list of words\n",
    "    vocab.update(text)\n",
    "    return text\n",
    "\n",
    "def process_comments(list_sentences, lower=False):\n",
    "    comments = []\n",
    "    for text in tqdm(list_sentences):\n",
    "        txt = text_to_wordlist(text, lower=lower)\n",
    "        comments.append(txt)\n",
    "    return comments\n",
    "\n",
    "\n",
    "list_sentences = list(df[\"headline_text\"].fillna(\"NAN_WORD\").values)\n",
    "\n",
    "\n",
    "comments = process_comments(list_sentences, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aba', 'decides', 'against', 'community', 'broadcasting', 'licence', 'act', 'fire', 'witnesses', 'must', 'be', 'aware', 'of', 'defamation', 'a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit', 'air', 'nz', 'staff', 'in']\n",
      "7105908\n"
     ]
    }
   ],
   "source": [
    "flat_list =[item for sublist in comments for item in sublist]\n",
    "print(flat_list[0:25])\n",
    "\n",
    "new=flat_list[15:20]\n",
    "n_words = len(flat_list)\n",
    "print(n_words)\n",
    "max_len= 75\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in new]],\n",
    "                            padding=\"post\", value=0, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||Prediction\n",
      "aba            : B-geo\n",
      "decides        : O    \n",
      "against        : O    \n",
      "community      : O    \n",
      "broadcasting   : O    \n",
      "licence        : B-geo\n",
      "act            : B-geo\n",
      "fire           : B-geo\n",
      "witnesses      : B-geo\n",
      "must           : B-geo\n",
      "be             : B-geo\n",
      "aware          : B-geo\n",
      "of             : B-geo\n",
      "defamation     : B-geo\n",
      "a              : B-geo\n",
      "g              : B-geo\n",
      "calls          : B-geo\n",
      "for            : B-geo\n",
      "infrastructure : B-geo\n",
      "protection     : B-geo\n",
      "summit         : B-geo\n",
      "air            : B-geo\n",
      "nz             : B-geo\n",
      "staff          : B-geo\n",
      "in             : B-geo\n",
      "aust           : B-geo\n",
      "strike         : B-geo\n",
      "for            : B-geo\n",
      "pay            : B-geo\n",
      "rise           : B-geo\n",
      "air            : B-geo\n",
      "nz             : B-geo\n",
      "strike         : B-geo\n",
      "to             : B-geo\n",
      "affect         : B-geo\n",
      "australian     : B-geo\n",
      "travellers     : B-geo\n",
      "ambitious      : B-geo\n",
      "olsson         : B-geo\n",
      "wins           : B-geo\n",
      "triple         : B-geo\n",
      "jump           : B-geo\n",
      "antic          : B-geo\n",
      "delighted      : B-geo\n",
      "with           : B-geo\n",
      "record         : B-geo\n",
      "breaking       : B-geo\n",
      "barca          : B-geo\n",
      "aussie         : B-geo\n",
      "qualifier      : B-geo\n",
      "stosur         : B-geo\n",
      "wastes         : B-geo\n",
      "four           : B-geo\n",
      "memphis        : B-geo\n",
      "match          : B-geo\n",
      "aust           : B-geo\n",
      "addresses      : B-geo\n",
      "un             : B-geo\n",
      "security       : B-geo\n",
      "council        : B-geo\n",
      "over           : B-geo\n",
      "iraq           : B-geo\n",
      "australia      : B-geo\n",
      "is             : B-geo\n",
      "locked         : B-geo\n",
      "into           : B-geo\n",
      "war            : B-geo\n",
      "timetable      : B-geo\n",
      "opp            : B-geo\n",
      "australia      : B-geo\n",
      "to             : B-geo\n",
      "contribute     : B-geo\n",
      "10             : B-geo\n",
      "million        : B-geo\n",
      "in             : B-geo\n"
     ]
    }
   ],
   "source": [
    "p1 = model.predict(np.array([x_test_sent[0]]))\n",
    "p1 = np.argmax(p1, axis=-1)\n",
    "print(\"{:15}||{}\".format(\"Word\", \"Prediction\"))\n",
    "word_new=[]\n",
    "pred_new=[]\n",
    "for w, pred in zip(flat_list, p1[0]):\n",
    "    print(\"{:15}: {:5}\".format(w, tags[pred]))\n",
    "    word_new.append(w)\n",
    "    pred_new.append(tags[pred])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Tag your new data!\n",
    "\n",
    "Create a modification to the **ent_tagger function** that combines words and tags from your original dataset. Now allow the function to also load new text from your new data set, and output the tags predicted from your trained model alongside the text. Make your function load five random texts from your data and output the tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aba', 'B-geo'), ('decides', 'O'), ('against', 'O'), ('community', 'O'), ('broadcasting', 'O'), ('licence', 'B-geo'), ('act', 'B-geo'), ('fire', 'B-geo'), ('witnesses', 'B-geo'), ('must', 'B-geo'), ('be', 'B-geo'), ('aware', 'B-geo'), ('of', 'B-geo'), ('defamation', 'B-geo'), ('a', 'B-geo'), ('g', 'B-geo'), ('calls', 'B-geo'), ('for', 'B-geo'), ('infrastructure', 'B-geo'), ('protection', 'B-geo'), ('summit', 'B-geo'), ('air', 'B-geo'), ('nz', 'B-geo'), ('staff', 'B-geo'), ('in', 'B-geo'), ('aust', 'B-geo'), ('strike', 'B-geo'), ('for', 'B-geo'), ('pay', 'B-geo'), ('rise', 'B-geo'), ('air', 'B-geo'), ('nz', 'B-geo'), ('strike', 'B-geo'), ('to', 'B-geo'), ('affect', 'B-geo'), ('australian', 'B-geo'), ('travellers', 'B-geo'), ('ambitious', 'B-geo'), ('olsson', 'B-geo'), ('wins', 'B-geo'), ('triple', 'B-geo'), ('jump', 'B-geo'), ('antic', 'B-geo'), ('delighted', 'B-geo'), ('with', 'B-geo'), ('record', 'B-geo'), ('breaking', 'B-geo'), ('barca', 'B-geo'), ('aussie', 'B-geo'), ('qualifier', 'B-geo'), ('stosur', 'B-geo'), ('wastes', 'B-geo'), ('four', 'B-geo'), ('memphis', 'B-geo'), ('match', 'B-geo'), ('aust', 'B-geo'), ('addresses', 'B-geo'), ('un', 'B-geo'), ('security', 'B-geo'), ('council', 'B-geo'), ('over', 'B-geo'), ('iraq', 'B-geo'), ('australia', 'B-geo'), ('is', 'B-geo'), ('locked', 'B-geo'), ('into', 'B-geo'), ('war', 'B-geo'), ('timetable', 'B-geo'), ('opp', 'B-geo'), ('australia', 'B-geo'), ('to', 'B-geo'), ('contribute', 'B-geo'), ('10', 'B-geo'), ('million', 'B-geo'), ('in', 'B-geo')]\n"
     ]
    }
   ],
   "source": [
    "# def ent_tagger_new(self):\n",
    "list=[]\n",
    "for i in range(0,len(word_new)):\n",
    "    list.append((word_new[i],pred_new[i]))\n",
    "print(list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
